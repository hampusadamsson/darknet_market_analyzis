{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import itertools\n",
    "import matplotlib\n",
    "from time import time\n",
    "# from pandas.tools.plotting import table\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import f1_score, adjusted_rand_score, accuracy_score\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, SparsePCA\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.decomposition import PCA, NMF, IncrementalPCA, FastICA, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.manifold import Isomap, MDS, LocallyLinearEmbedding, TSNE\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphabay : 15222 samples, \n",
      " 12 mains,  48 subs\n",
      "poseidon : 3688 samples, \n",
      " 10 mains,  36 subs\n",
      "silkroad : 2137 samples, \n",
      " 1 mains,  22 subs\n"
     ]
    }
   ],
   "source": [
    "category = {}\n",
    "category_sub = {}\n",
    "description = {}\n",
    "\n",
    "for nm in [\"alphabay\", \"poseidon\", \"silkroad\"]:\n",
    "    description[nm], category[nm], category_sub[nm] = pickle.load(open(\"data/meta/{}_dataset.p\".format(nm), \"rb\"))    \n",
    "    print(nm, \":\", len(description[nm]), \"samples, \\n\", len(set(category[nm])),\"mains, \", len(set(category_sub[nm])), \"subs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def get_c(c):\n",
    "#     if \"Carding\" in c:\n",
    "#         return(\"CardedItems\")\n",
    "#     if \"Money\" in c or \"Electronics\" in c:\n",
    "#         return(\"CounterfeitItems\")\n",
    "#     if \"Fraud\" in c  or \"Accounts\" in c or \"Accohackunts & Bank Drops\" in c:\n",
    "#         return(\"Fraud\")\n",
    "#     if \"Software\" in c:\n",
    "#         return(\"Software&Malware\")\n",
    "#     if \"Erotica\" in c or \"NB\" in c or \"Other\" in c:\n",
    "#         return(\"OtherListings\")\n",
    "#     if \"Security software\" in c or \"Security & Anonymity\" in c:\n",
    "#         return(\"Security&Hosting\")\n",
    "#     if \"Non-lethal weapons\" in c or \"Pistols\" in c:\n",
    "#         return(\"Weapons\")\n",
    "#     if \"Hash\" in c or \"Ecstasy\" in c or \"Shake/trim\" in c or \"Drugs\" in c or \"Speed\" in c or \"Pills\" in c or \"Weed\" in c or \"Buprenorphine\" in c or \"Cocaine\" in c or  \"Mushrooms\" in c or \"MDMA\" in c or \"Concentrates\" in c or \"Cannabis\" in c or \"Codeine\" in c or \"Prescription\" in c or \"Stimulants\" in c or \"Benzos\" in c:\n",
    "#         return(\"Drugs&Chemicals\")\n",
    "#     if \"Digital products\" in c or \"IT\" in c:\n",
    "#         return(\"DigitalProducts\")\n",
    "# #     if \"\" in c:\n",
    "# #         return(\"Jewels&Gold\")\n",
    "# #     if \"\" in c:\n",
    "# #         return(\"Services\")\n",
    "#     if \"Hacking\" in c or \"Making money\" in c or \"Info / Guides / eBooks\" in c:\n",
    "#         return(\"Guides&Tutorials\")\n",
    "#     else:\n",
    "#         return('Er')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def render_mpl_table(df):\n",
    "#     size = (np.array(df.shape[::-1]) + np.array([0, 1])) * np.array([2.1, .6])\n",
    "#     fig, ax = plt.subplots(figsize=size)\n",
    "#     ax.axis('off')\n",
    "#     mpl_table = ax.table(cellText=df.values, bbox=[0, 0, 1, 1], colLabels=df.columns)\n",
    "#     mpl_table.auto_set_font_size(False)\n",
    "#     mpl_table.set_fontsize(12)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from IPython.display import display\n",
    "# from pandas import DataFrame\n",
    "# import pandas as pd\n",
    "\n",
    "# category_ab_main = [x.get(\"category\").split(\"/\")[0] for x in data_ab]\n",
    "# category_ab = [x.get(\"category\") for x in data_ab]\n",
    "# category_po = [x.get(\"category\") for x in data_po]\n",
    "\n",
    "# category_po_main = []\n",
    "# for c in category_po:\n",
    "#     category_po_main.append(get_c(c))\n",
    "    \n",
    "# for i,c in enumerate(data_ab):\n",
    "#     c['category_main'] = category_ab_main[i]\n",
    "# #     c['category_main'] = category_po_main[i]\n",
    "    \n",
    "# df = DataFrame(index=range(13))\n",
    "# for x in list(set(category_ab_main)):\n",
    "#     df[x] = pd.Series([y.split(\"/\")[1][:17] for y in list(set(category_ab)) if x in y])\n",
    "    \n",
    "# df = df.replace(np.nan, '', regex=True)\n",
    "# display(df)\n",
    "# render_mpl_table(df)\n",
    "# # 2\n",
    "\n",
    "# df = DataFrame(index=range(17))\n",
    "# for x in list(set(category_po_main)):\n",
    "#     df[x] = pd.Series([y[:14] for y in list(set(category_po)) if x==get_c(y)])\n",
    "# df = df.replace(np.nan, '', regex=True)\n",
    "# display(df)\n",
    "# render_mpl_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "class keyword_estimator:\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        self.prio_common = True\n",
    "    \n",
    "    def fit(self, inp, target):\n",
    "        self.cv = CountVectorizer(stop_words='english', max_df=0.85)\n",
    "\n",
    "        res = self.cv.fit_transform(inp)\n",
    "        self.word_m = res.sum(0)\n",
    "        \n",
    "        # Normalize\n",
    "        row_sums = self.word_m.sum(axis=1)\n",
    "        self.word_m = (self.word_m.T / row_sums).T\n",
    "\n",
    "        # Collect keywords for category\n",
    "        self.keywords = {}\n",
    "        for sub in set(target):\n",
    "            try:\n",
    "                self.keywords[sub] = get_words(sub)  \n",
    "            except:\n",
    "                self.keywords[sub] = [\"none\"]\n",
    "\n",
    "    #\n",
    "    # internal function: collects words for sub-categories keys\n",
    "    #\n",
    "    def get_words(self, sub):\n",
    "        # Map words-occurance using CV's vocabulary_\n",
    "        occurance = {}\n",
    "        for x in self.cv.vocabulary_.items():\n",
    "            occurance[x[0]] = self.word_m.item(x[1])\n",
    "\n",
    "        # Sort dict to list\n",
    "        d_key = [k for k in sorted(occurance, key=lambda k: occurance[k], reverse=True)]\n",
    "        return d_key\n",
    "    \n",
    "    #\n",
    "    # Score the estimator\n",
    "    #\n",
    "    def score(self, y_pred, y_true, f1=False):\n",
    "        if not f1:\n",
    "            return(sklearn.metrics.accuracy_score(y_pred, y_true))\n",
    "        else:\n",
    "            return({\"Accuracy:\":sklearn.metrics.accuracy_score(y_pred, y_true),\n",
    "                    \"weighted\":f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "                    \"macro\":f1_score(y_true, y_pred, average=\"macro\"),\n",
    "                    \"micro\":f1_score(y_true, y_pred, average=\"micro\")})\n",
    "    \n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    def get_params(self, deep=False):\n",
    "        param = {\"weighted\":self.prio_common}\n",
    "        return(param)\n",
    "    \n",
    "    #\n",
    "    # Do the predictions\n",
    "    #\n",
    "    def predict(self, inp):\n",
    "#         print(self.keywords.keys())\n",
    "        res = []\n",
    "        for inp_ in inp:\n",
    "            inp_ = word_tokenize(inp_)\n",
    "            score = {}\n",
    "\n",
    "            # iterate over all categories\n",
    "            for keyw in self.keywords.keys():\n",
    "                # Collect words related to the category\n",
    "                for i, x in enumerate(self.keywords[keyw]):\n",
    "                    if not self.prio_common:\n",
    "                        i = 1\n",
    "                        \n",
    "                    if x in inp_:\n",
    "                        try:\n",
    "                            score[keyw]+=1./(i+1)\n",
    "                        except:\n",
    "                            score[keyw]=1./(i+1)\n",
    "            try:\n",
    "                res.append(max(score, key=lambda k:score[k]))\n",
    "            except:\n",
    "                res.append(\"OtherListings\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #\n",
    "# # Testing the keyword estimator\n",
    "# #\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(description[domain], problem, test_size=0.25, random_state=1)\n",
    "\n",
    "# X_train = description[domain]\n",
    "# y_train = problem\n",
    "\n",
    "# clf = keyword_estimator()\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.prio_common=True\n",
    "# pred = clf.predict(X_test)\n",
    "\n",
    "# clf.score(pred, y_test, f1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# reduce dimension\n",
    "# Input: input, output_dimension \n",
    "# Out:   X, reduction_object\n",
    "#\n",
    "\n",
    "def red_dim(inp, dim_out):\n",
    "    pca = TruncatedSVD(n_components=dim_out, random_state=1)\n",
    "    pca.fit(inp)\n",
    "    X = pca.transform(inp)\n",
    "    return X, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #\n",
    "# # Oversampling\n",
    "# #\n",
    "\n",
    "# def oversample(di, cat, nr):\n",
    "#     tmp=[]\n",
    "#     for x in range(nr):\n",
    "#         new = di.get(cat)[x%len(di.get(cat))]\n",
    "#         tmp.append(new)\n",
    "        \n",
    "#     di[cat] = di.get(cat) + tmp\n",
    "    \n",
    "\n",
    "# max_sample = max([[x, category.count(x)] for x in set(category)], key=lambda x: x[1])[1]\n",
    "# print(max_sample)\n",
    "\n",
    "# #\n",
    "# # Add al description to a dictionary\n",
    "# #\n",
    "\n",
    "# di = {}\n",
    "# for c,d in zip(category, description):\n",
    "#     try:\n",
    "#         di.get(c).append(d)\n",
    "#     except:\n",
    "#         di[c] = []\n",
    "#         di.get(c).append(d)\n",
    "\n",
    "# for x in di.keys():\n",
    "#     if len(di.get(x)) <= max_sample:\n",
    "#         oversample(di, x, max_sample-len(di.get(x)))\n",
    "        \n",
    "# category = []\n",
    "# description = []\n",
    "\n",
    "# for x in di.keys():\n",
    "#     description += di.get(x)\n",
    "#     category += [x]*len(di.get(x))\n",
    "\n",
    "# print(len(category))\n",
    "# print(len(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ### FOR ANOMALY DETECTION\n",
    "\n",
    "# def anom():\n",
    "#     weapon = [x for x in category if u'Weapons' in x]\n",
    "#     weapon_binary = [u'Weapons' in x for x in category]\n",
    "\n",
    "#     print \"weapons:\", len(weapon), \"(\"+str(len(weapon))+\")\"\n",
    "#     print float(len(weapon))/float(len(category)-len(weapon))*100, \"% weapons\"\n",
    "\n",
    "#     w_description = []\n",
    "#     nw_category = copy(category)\n",
    "#     nw_description = copy(description)\n",
    "\n",
    "#     for _ in [1,2]: # remove this for \n",
    "#         for i,x in enumerate(nw_category):\n",
    "#             if u'Weapons' in x:\n",
    "#                 w_description.append(nw_description[i])\n",
    "#                 nw_category.pop(i)\n",
    "#                 nw_description.pop(i)\n",
    "                \n",
    "#     return weapon, weapon_binary, w_description, nw_category, nw_description\n",
    "\n",
    "# # weapon, weapon_binary, w_description, nw_category, nw_description = anom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "# vect = CountVectorizer(tokenizer=LemmaTokenizer())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #\n",
    "# # count vectoriser - \n",
    "# # Ret: matrix containing occurance of word in document \n",
    "# #\n",
    "# # print(X_train_counts) \n",
    "# #   (document, word_id, occurances)\n",
    "# #   (0, 5976)\t2\n",
    "# #   (0, 15089)\t1\n",
    "# #   (0, 7283)\t1\n",
    "# #\n",
    "# def count_word_matrix():\n",
    "#     global X_train_counts\n",
    "#     count_vect = CountVectorizer(tokenizer=LemmaTokenizer()) #stop_words = 'english', min_df=0.0008\n",
    "#     X_train_counts = count_vect.fit_transform(description)\n",
    "#     print(X_train_counts.shape)\n",
    "    \n",
    "# # count_word_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Feature impact\n",
    "#\n",
    "\n",
    "\n",
    "def feat_impact(inp, start=10, stop=1000, step=50):    \n",
    "    %matplotlib inline\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "    cv = []\n",
    "    cv.append((CountVectorizer(),'default'))\n",
    "    cv.append((CountVectorizer(stop_words='english'),'stopwords'))\n",
    "    cv.append((CountVectorizer(min_df=2),'minDF=4'))\n",
    "    cv.append((CountVectorizer(tokenizer=LemmaTokenizer()),'Lemma'))\n",
    "    cv.append((CountVectorizer(min_df=8),'minDF=6'))\n",
    "    cv.append((CountVectorizer(min_df=0.01),'minDF=0.01'))\n",
    "    cv.append((CountVectorizer(max_df=0.5),'maxDF=0.5'))\n",
    "    cv.append((CountVectorizer(max_df=0.8),'maxDF=0.8'))\n",
    "\n",
    "\n",
    "    res = [[] for _ in range(len(cv))]\n",
    "    rangen = range(start, stop, step)\n",
    "\n",
    "    for i in rangen:\n",
    "        x = description[inp][:i]\n",
    "        for ind, (cv_,lb) in enumerate(cv):\n",
    "            feat = cv_.fit_transform(x).shape[1]\n",
    "            res[ind].append(feat)\n",
    "\n",
    "    plt.xlabel(\"Number of documents\")\n",
    "    plt.ylabel(\"Number of unique words\")\n",
    "\n",
    "    for ind, res_ in enumerate(res):\n",
    "        plt.plot(rangen, res_, label=cv[ind][1])\n",
    "\n",
    "    plt.title(inp)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "    \n",
    "# feat_impact(\"silkroad\")\n",
    "# feat_impact(\"poseidon\")\n",
    "# feat_impact(\"alphabay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Remove numbers from a sample\n",
    "#\n",
    "def remove_numbers(str):\n",
    "    result = ''.join(i for i in str if not i.isdigit())\n",
    "    return(result)\n",
    "\n",
    "\n",
    "def do_remove():\n",
    "    for x in description.keys():\n",
    "        tmp = description[x]\n",
    "        for y in tqdm(range(len(tmp))):\n",
    "            description[x][y] = remove_numbers(description[x][y])\n",
    "\n",
    "# do_remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TFIDF w/o using countvectorizer (Equivalent to CountVectorizer + TfidfTransformer)\n",
    "#\n",
    "def tfidf(inp):\n",
    "#     tfidf = HashingVectorizer(stop_words='english', n_features=100000)\n",
    "#     global voce\n",
    "    tfidf = TfidfVectorizer(use_idf=True,\n",
    "                            stop_words='english',\n",
    "                            min_df = 3,\n",
    "                            max_df= 0.25,\n",
    "#                             vocabulary=voce,\n",
    "                            sublinear_tf=True)\n",
    "#     tfidf.fit(inp)\n",
    "#     voce = tfidf.vocabulary_\n",
    "    return tfidf.fit_transform(inp), tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #\n",
    "# # Problem and target \n",
    "# # #\n",
    "# domain = \"poseidon\"\n",
    "# problem = category[domain] #[190:3690]\n",
    "# input_data, vocab = tfidf(description[domain]) #[190:3690])\n",
    "# problem_numeric = [sorted(list(set(category[domain]))).index(x) for x in problem]\n",
    "\n",
    "# print(\"Samples:\", len(problem))\n",
    "# print(\"Features:\", input_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# pprint(sorted(list(set(category[\"alphabay\"]))))\n",
    "# pprint(sorted(list(set(category[\"poseidon\"]))))\n",
    "# print(len(category[\"poseidon\"]))\n",
    "# # description[\"Poseidon\"].append(\"Jewel&Gold\")\n",
    "# # description[\"Poseidon\"].append(\"Services\")\n",
    "# # category[\"Poseidon\"].append(\"Jewels&Gold\")\n",
    "# # category[\"Poseidon\"].append(\"Services\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# clf = list of estimators\n",
    "#\n",
    "\n",
    "def create_clf(inp_):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inp_, problem_numeric, test_size=0.25, random_state=1)\n",
    "\n",
    "    clf = VotingClassifier([('SVC_rbf',SVC()),\n",
    "                            ('Naibe_B',sklearn.naive_bayes.BernoulliNB()),\n",
    "                            ('SVC_linear', sklearn.svm.LinearSVC()),\n",
    "                            ('Ridge',sklearn.linear_model.RidgeClassifier()),\n",
    "                            ('KNN',sklearn.neighbors.KNeighborsClassifier()),\n",
    "                            ('SGD',SGDClassifier())])\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    return(clf)\n",
    "\n",
    "#\n",
    "# ---\n",
    "#\n",
    "\n",
    "# input_ = input_data\n",
    "# clf = create_clf(input_)\n",
    "# clfD = (clf, input_, \"none\", \"{} - Default\".format(input_.shape[1]))\n",
    "\n",
    "# clfs = [clfD]\n",
    "# #clfs = []\n",
    "# for x in tqdm([1000, 300, 5, 3]):\n",
    "#     input_, pca = red_dim(input_data, x)\n",
    "#     clf = create_clf(input_)\n",
    "#     clfs.append((clf, input_, pca, \"{}dim\".format(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #\n",
    "# # Swap datasest \n",
    "# #\n",
    "# for i,x in enumerate(clfs):\n",
    "#     clf_, old_, pca_, nm = x\n",
    "#     print(old_.shape[1], \"->\", input_data.shape[1])\n",
    "#     input_ = pca_.transform(input_data)\n",
    "# #     input_, pca = red_dim(input_data, old_.shape[1])\n",
    "# #     input_ = input_data\n",
    "#     clfs[i] = (clf_, input_, pca_, nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Plot result of multiple classifiers (clfs)\n",
    "#\n",
    "\n",
    "\n",
    "def plot_clfs(clf_list):    \n",
    "    %matplotlib inline\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "    fig = plt.figure(figsize=(25,7))\n",
    "\n",
    "    for ind, clf_tup in tqdm(enumerate(clf_list)):\n",
    "        clf_, input_, pca_, names = clf_tup\n",
    "        _, X_test, _, y_test = train_test_split(input_, problem_numeric, test_size=0.25, random_state=1)\n",
    "        \n",
    "        clf_count = np.arange(len(clf_.estimators))\n",
    "        clf_scores = [est.score(X_test, y_test) for est in clf_.estimators_]\n",
    "        \n",
    "        clf_names = [n[0] for n in clf_.estimators]\n",
    "        plt.bar(clf_count+((.8/len(clf_list)))*ind-.35, clf_scores, width=.8/(len(clf_list)), label=names)\n",
    "\n",
    "        plt.xticks(clf_count, clf_names)\n",
    "        \n",
    "    fig.autofmt_xdate()\n",
    "    plt.legend(ncol=3)\n",
    "    plt.title(domain)\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlabel(\"Estimator\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    fig.show()\n",
    "    plt.show()\n",
    "\n",
    "# plot_clfs(clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def eval_clf(clf_inp):    \n",
    "    %matplotlib inline\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "    classifier, input_, pca_, names = clf_inp\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_, problem_numeric, test_size=0.25, random_state=1)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    clf_count = len(classifier.estimators)\n",
    "\n",
    "    for i, clfs_ in tqdm(enumerate(classifier.estimators_)):\n",
    "        y_pred = clfs_.predict(X_test)\n",
    "\n",
    "        scores = []\n",
    "        scores.append(clfs_.score(X_test, y_test))\n",
    "        for avg in ['weighted', 'macro', 'micro']:\n",
    "            scores.append(f1_score(y_test, y_pred, labels=list(set(problem_numeric)), average=avg))\n",
    "\n",
    "        plt.bar(np.arange(4)+(.1*i)-0.25, scores, width=.1, label=classifier.estimators[i][0])\n",
    "        plt.title(names)\n",
    "\n",
    "    plt.xticks(range(4), ['Accuracy','F1 Micro','F1 Macro','F1 Weighted'])\n",
    "    plt.legend(loc='upper left',ncol=2)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlabel(\"Evaluation method\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    fig.show()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# for clf_ in clfs:\n",
    "#     eval_clf(clf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Plot confussion matrix\n",
    "#\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(clf, normalize=False):    \n",
    "    %matplotlib inline\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "    classes = sorted(list(set(problem)))\n",
    "    classifier, input_, pca_, names = clf\n",
    "    # classifier = classifier.estimators_[4]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_, problem_numeric, test_size=0.25, random_state=1)\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(\"Accuracy:\",classifier.score(X_test, y_test))\n",
    " \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    " \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title = 'Normalized confusion matrix \\n {}'.format(names)\n",
    "    else:\n",
    "        title = 'Confusion matrix \\n {}'.format(names)\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    fig.set_size_inches(10, 10) #(22, 14)\n",
    "\n",
    "    cmap = plt.cm.Blues\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0., vmax=cm.max())\n",
    "    plt.title(title)\n",
    "#     plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    thresh = cm.max() * 0.85\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if cm[i, j] < 0.01:\n",
    "            continue\n",
    "        if normalize:\n",
    "            thresh = .64\n",
    "            val = str(round(cm[i, j]*100,2)).split(\".\")[0]+'%'\n",
    "        else:\n",
    "            val = cm[i, j]\n",
    "            \n",
    "        if cm[i, j] > thresh:\n",
    "            cl=\"white\"\n",
    "        else:\n",
    "            cl=\"black\"\n",
    "            \n",
    "        plt.text(j, i, val, horizontalalignment=\"center\", color=cl) #, alpha=cm[i,j]\n",
    "\n",
    "#     plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "# plot_confusion_matrix(clfs[0])\n",
    "# plot_confusion_matrix(clfs[0], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # input_data = [X]\n",
    "# X_train, y_train = X_train_tfidf_nw, nw_category\n",
    "# X_train, y_train = X_train_tfidf_w, weapon\n",
    "\n",
    "# X_test, y_test = X_train_tfidf, category\n",
    "# X_test_w, y_test_w = X_train_tfidf_w, weapon\n",
    "# X_test_nw, y_test_nw = X_train_tfidf_nw, nw_category\n",
    "\n",
    "# clf = OneClassSVM()\n",
    "# parameters = {'kernel':('linear', 'rbf', 'sigmoid'), 'gamma':np.arange(0,1,0.1), 'nu':np.arange(0.01,1,0.1)}\n",
    "# outl = GridSearchCV(clf, parameters, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# outl.fit(X_train, y_train)\n",
    "\n",
    "# # ----\n",
    "# inliers = float(outl.predict(X_test).tolist().count(1))/len(y_test)*100\n",
    "# print(\"Inliers (All): {}%\".format(round(inliers,1)))\n",
    "\n",
    "# inliers = float(outl.predict(X_test_w).tolist().count(1))/len(y_test_w)*100\n",
    "# print(\"Inliers (Weapons): {}%\".format(round(inliers,1)))\n",
    "\n",
    "# inliers = float(outl.predict(X_test_nw).tolist().count(1))/len(y_test_nw)*100\n",
    "# print(\"Inliers (NO Weapons): {}%\".format(round(inliers,1)))\n",
    "\n",
    "# print(outl.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X_train, y_train = X_train_tfidf_w, weapon\n",
    "\n",
    "# X_test, y_test = X_train_tfidf, category\n",
    "# X_test_w, y_test_w = X_train_tfidf_w, weapon\n",
    "# X_test_nw, y_test_nw = X_train_tfidf_nw, nw_category\n",
    "\n",
    "# # be = OneClassSVM()\n",
    "# # be = OneClassSVM(nu=0.01, gamma=0.15, kernel='linear')\n",
    "# be = IsolationForest()\n",
    "# # be = GaussianMixture(n_components=33)\n",
    "# # be = KernelDensity()\n",
    "\n",
    "\n",
    "# be.fit(X_train, y_train)\n",
    "\n",
    "# inliers = float(be.predict(X_test).tolist().count(1))/len(y_test)*100\n",
    "# print(\"Inliers (All): {}%\".format(round(inliers,1)))\n",
    "\n",
    "# inliers = float(be.predict(X_test_w).tolist().count(1))/len(y_test_w)*100\n",
    "# print(\"Inliers (Weapons): {}%\".format(round(inliers,1)))\n",
    "\n",
    "# inliers = float(be.predict(X_test_nw).tolist().count(1))/len(y_test_nw)*100\n",
    "# print(\"Inliers (NO Weapons): {}%\".format(round(inliers,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clf = sklearn.neighbors.KNeighborsClassifier().fit(X_train, y_train)\n",
    "# print(\"Accuracy:\",clf.score(X_test,y_test))\n",
    "\n",
    "# from pandas import DataFrame\n",
    "# col = []\n",
    "# for i, da in enumerate(y_test):\n",
    "#     if b'Wea' in da:\n",
    "#         pred_y = clf.predict(X_test[i])[0]\n",
    "#         ol = 1!=outl.predict(X_test[i])[0]\n",
    "#         col.append([da,pred_y,ol])\n",
    "\n",
    "# df = DataFrame(col,columns=[\"Y_true\",\"Y_predicted\",\"outlier\"])\n",
    "# print(df.loc(df[\"outlier\"]==True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(\"Hits:\",df.shape[0])\n",
    "# print(\"Wrong pred:\",df.loc[df[\"Y_true\"]!=df[\"Y_predicted\"]].shape[0])\n",
    "# print(\"Correct pred:\",df.loc[df[\"Y_true\"]==df[\"Y_predicted\"]].shape[0])\n",
    "# print(\"Outliers:\",df.loc[df[\"outlier\"]==1].shape[0])\n",
    "# print(\"Outliers & wrong prediction:\",df.loc[df[\"outlier\"]==1][df[\"Y_true\"]!=df[\"Y_predicted\"]].shape[0])\n",
    "# df.loc[df[\"Y_true\"]!=df[\"Y_predicted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Plot X (3D)  \n",
    "# Input: X (global)\n",
    "#\n",
    "\n",
    "def plot_data_3d(text=False):    \n",
    "    %matplotlib inline\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "    X3d,_ = red_dim(input_data, 3)    \n",
    "    fig = plt.figure(1, figsize=(10, 8))\n",
    "    ax = Axes3D(fig)\n",
    "    y = problem\n",
    "    for name, label in zip(list(set(problem)), list(set(y))):        \n",
    "        ax.scatter(X3d[[label==t for t in y], 0], \n",
    "                  X3d[[label==t for t in y], 1],\n",
    "                  X3d[[label==t for t in y], 2], \n",
    "                   label=name, \n",
    "                   s=3,\n",
    "                   alpha=1)\n",
    "   \n",
    "        if text:\n",
    "            ax.text3D(X3d[[label==t for t in y], 0].mean(),\n",
    "                      X3d[[label==t for t in y], 1].mean(),\n",
    "                      X3d[[label==t for t in y], 2].mean(), name)          \n",
    "\n",
    "    plt.title(domain)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "#     plt.legend(loc='upper left', ncol=2)\n",
    "    ax.set_xlabel('PCA 0')\n",
    "    ax.set_ylabel('PCA 1')\n",
    "    ax.set_zlabel('PCA 2')\n",
    "    plt.show()\n",
    "    \n",
    "# plot_data_3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Plot X (2D)  \n",
    "# Input: X2d (global)\n",
    "#\n",
    "    \n",
    "def plot_data_2d(text=False):\n",
    "    %matplotlib inline\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "    X2d,_ = red_dim(input_data, 2)\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    y = problem\n",
    "    \n",
    "    for name in list(set(y)):\n",
    "        plt.scatter(X2d[[name==t for t in y], 0],\n",
    "                    X2d[[name==t for t in y], 1],\n",
    "                    label=name, \n",
    "                    s=10,\n",
    "                    alpha=1,\n",
    "                   )\n",
    "        if text:\n",
    "            plt.text(X2d[[name==t for t in y], 0].mean(),\n",
    "                     X2d[[name==t for t in y], 1].mean(),\n",
    "                     name)\n",
    "    plt.title(domain)\n",
    "    plt.xlabel(\"PCA0\")\n",
    "    plt.ylabel(\"PCA1\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "#     plt.legend(loc='upper left', ncol=2)\n",
    "    plt.show()\n",
    "    \n",
    "# plot_data_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Scree plot\n",
    "#\n",
    "\n",
    "def scree(dim=1000, threshold=.9):\n",
    "    matplotlib.rcParams.update({'font.size': 13})\n",
    "    xRed_, pca2 = red_dim(input_data, dim)\n",
    "\n",
    "\n",
    "    plt.ylim(0)\n",
    "#     plt.text(1,0.85,\"Number of components\\n\"+str(input_data.shape[1]))\n",
    "    var_rat = np.cumsum(pca2.explained_variance_ratio_)\n",
    "    var = np.cumsum(pca2.explained_variance_)\n",
    "    var = [float(i)/pca2.explained_variance_[0] for i in pca2.explained_variance_]\n",
    "\n",
    "    %matplotlib inline\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "    plt.ylabel(\"Variance\")\n",
    "    plt.xlabel(\"Components\")\n",
    "    plt.title(\"{}\\n{} Features\".format(domain, input_data.shape[1]))\n",
    "    plt.plot(var_rat, label=\"Cumulative variance ratio\")\n",
    "    plt.plot(var, label=\"Normalized variance ratio\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    \n",
    "    for i, x in enumerate(var_rat):\n",
    "        if x > threshold:\n",
    "            return i\n",
    "    return dim\n",
    "        \n",
    "# scree(100, .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Problem and target \n",
    "#\n",
    "\n",
    "domain = \"alphabay\"\n",
    "problem = category[domain] #[190:3690]\n",
    "input_data, vocab = tfidf(description[domain]) #[190:3690])\n",
    "problem_numeric = [sorted(list(set(category[domain]))).index(x) for x in problem]\n",
    "\n",
    "print(\"Samples:\", len(problem))\n",
    "print(\"Features:\", input_data.shape[1])\n",
    "\n",
    "\n",
    "#\n",
    "# Visualize data\n",
    "#\n",
    "plot_data_3d()\n",
    "plot_data_2d()\n",
    "\n",
    "\n",
    "#\n",
    "# Check BOW-model feature restraints\n",
    "#\n",
    "\n",
    "feat_impact(domain, start=10, stop=2500, step=100)\n",
    "\n",
    "#\n",
    "# Check when exp. var > treshold (nm of useful features) \n",
    "#\n",
    "\n",
    "dims = scree(7500, .95)\n",
    "\n",
    "\n",
    "#\n",
    "# Train models \n",
    "#\n",
    "\n",
    "# clf = create_clf(input_data)\n",
    "# clfD = (clf, input_data, \"none\", \"{} - Default\".format(input_data.shape[1]))\n",
    "# clfs = [clfD]\n",
    "clfs = []\n",
    "for x in tqdm([dims, int(dims/2), 5, 3]):\n",
    "    input_, pca = red_dim(input_data, x)\n",
    "    clf = create_clf(input_)\n",
    "    clfs.append((clf, input_, pca, \"{}dim\".format(x)))\n",
    "    \n",
    "#\n",
    "# Display result\n",
    "#\n",
    "\n",
    "plot_clfs(clfs)\n",
    "\n",
    "\n",
    "#\n",
    "# Result F1 & Confusion matrix\n",
    "#\n",
    "\n",
    "for clf_ in clfs:\n",
    "    eval_clf(clf_)\n",
    "    plot_confusion_matrix(clf_, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
